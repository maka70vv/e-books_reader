{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Подготовка датасета",
   "id": "3f60b869b026352f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T08:17:36.296777Z",
     "start_time": "2025-03-28T08:17:24.553403Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "# Пути к файлам\n",
    "dataset_path = \"data/test/normal_voices\"\n",
    "xlsx_path = \"data/test/Speeches.xlsx\"\n",
    "\n",
    "# Читаем данные из Excel\n",
    "df = pd.read_excel(xlsx_path)\n",
    "\n",
    "# Загружаем аудиофайлы и их MFCC\n",
    "audio_data = {}\n",
    "for _, row in df.iterrows():\n",
    "    wav_file = row[\"file\"]\n",
    "    text = row[\"text\"]\n",
    "    audio_path = os.path.join(dataset_path, f\"{wav_file}.wav\")\n",
    "\n",
    "    if os.path.exists(audio_path):\n",
    "        y, sr = librosa.load(audio_path, sr=22050)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        audio_data[text] = (y, sr, mfcc)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/makarov/miniconda3/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Графовая модель на основе DTW",
   "id": "ea0f697142b914c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T08:26:31.405949Z",
     "start_time": "2025-03-28T08:26:14.214578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "# Функция поиска ближайшего совпадения\n",
    "def find_closest_match(text_mfcc, audio_data):\n",
    "    best_match = None\n",
    "    best_dist = float(\"inf\")\n",
    "\n",
    "    for text, (y, sr, mfcc) in audio_data.items():\n",
    "        dist, _ = fastdtw(text_mfcc.T, mfcc.T)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_match = y\n",
    "\n",
    "    return best_match\n",
    "\n",
    "# Генерация речи\n",
    "def generate_speech(text, audio_data):\n",
    "    # Если текст уже есть в аудиоданных, возвращаем его напрямую\n",
    "    if text in audio_data:\n",
    "        speech, sr, _ = audio_data[text]\n",
    "    else:\n",
    "        # Иначе ищем наиболее похожий фрагмент\n",
    "        ref_text = list(audio_data.keys())[0]  # Выбираем любой реальный образец\n",
    "        ref_audio, ref_sr, ref_mfcc = audio_data[ref_text]\n",
    "\n",
    "        # Берем MFCC случайного реального аудиофайла (чтобы избежать случайного шума)\n",
    "        text_mfcc = ref_mfcc\n",
    "        speech = find_closest_match(text_mfcc, audio_data)\n",
    "\n",
    "    # Сохраняем результат\n",
    "    sf.write(\"output_dtw.wav\", speech, 22050)\n",
    "\n",
    "# Пример вызова\n",
    "generate_speech(\"Привет общество\", audio_data)\n"
   ],
   "id": "d170011bf617216e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "WaveNet с обучением с нуля",
   "id": "7f173f71f3bd9da6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T08:24:45.740943Z",
     "start_time": "2025-03-28T08:22:32.847845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Пути к данным\n",
    "dataset_path = \"data/test/normal_voices\"\n",
    "xlsx_path = \"data/test/Speeches.xlsx\"\n",
    "\n",
    "# Читаем Excel-файл\n",
    "df = pd.read_excel(xlsx_path)\n",
    "\n",
    "# Модель WaveNet\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(WaveNet, self).__init__()\n",
    "        # Первый слой: 128 каналов на входе, 32 канала на выходе\n",
    "        self.conv1 = nn.Conv1d(n_input, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Второй слой: 32 канала на входе, n_output на выходе\n",
    "        self.conv2 = nn.Conv1d(32, n_output, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "# Генерация спектрограммы\n",
    "def audio_to_mel(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=22050)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    return mel\n",
    "\n",
    "# Обучение модели\n",
    "def train_wavenet(dataset_path, df):\n",
    "    n_input = 128  # Размерность Mel-спектрограммы (каналы)\n",
    "    n_output = 128  # Размерность выходных данных\n",
    "\n",
    "    model = WaveNet(n_input=n_input, n_output=n_output)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for _, row in df.iterrows():\n",
    "            audio_path = os.path.join(dataset_path, f\"{row['file']}.wav\")\n",
    "\n",
    "            if not os.path.exists(audio_path):\n",
    "                continue  # Пропустить, если файл не найден\n",
    "\n",
    "            mel = audio_to_mel(audio_path)\n",
    "            mel = torch.tensor(mel).unsqueeze(0).float()  # Добавляем batch dimension: (1, freq, time)\n",
    "\n",
    "            # Трансформируем в правильную форму: (batch_size, channels, time)\n",
    "            mel = mel.squeeze(0)  # Убираем лишнюю ось, теперь (freq, time)\n",
    "            mel = mel.unsqueeze(0)  # Добавляем ось batch size: (1, freq, time)\n",
    "\n",
    "            output = model(mel)\n",
    "\n",
    "            loss = criterion(output, mel)  # Выход должен совпадать с входом\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    return model\n",
    "\n",
    "# Запуск обучения\n",
    "model = train_wavenet(dataset_path, df)\n"
   ],
   "id": "cb02da692d5a92e9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/makarov/miniconda3/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.127837181091309\n",
      "Epoch 1, Loss: 3.5629818439483643\n",
      "Epoch 2, Loss: 2.5861129760742188\n",
      "Epoch 3, Loss: 2.2238385677337646\n",
      "Epoch 4, Loss: 2.280803918838501\n",
      "Epoch 5, Loss: 2.673626184463501\n",
      "Epoch 6, Loss: 2.1938414573669434\n",
      "Epoch 7, Loss: 2.382476329803467\n",
      "Epoch 8, Loss: 1.68925940990448\n",
      "Epoch 9, Loss: 1.6742852926254272\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T08:25:12.947199Z",
     "start_time": "2025-03-28T08:25:10.204197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Преобразование текста в мел-спектрограмму (для упрощения)\n",
    "def text_to_mel_spectrogram(text):\n",
    "    return np.random.random((128, 50))  # Заглушка для генерации случайной спектрограммы\n",
    "\n",
    "# Используем WaveNet для генерации аудио\n",
    "def text_to_audio(text, model, sample_rate=22050):\n",
    "    mel_spectrogram = text_to_mel_spectrogram(text)  # Преобразуем текст в мел-спектрограмму\n",
    "    mel_tensor = torch.tensor(mel_spectrogram).unsqueeze(0).float()  # Преобразуем в тензор\n",
    "    output = model(mel_tensor)  # Генерируем выход модели\n",
    "\n",
    "    audio = mel_to_audio(output.squeeze(0).detach().numpy(), sample_rate)  # Преобразуем спектрограмму в аудио\n",
    "    return audio\n",
    "\n",
    "# Преобразование мел-спектрограммы в аудио\n",
    "def mel_to_audio(mel, sample_rate=22050):\n",
    "    return librosa.feature.inverse.mel_to_audio(mel, sr=sample_rate)\n",
    "\n",
    "# Сохранение аудио в файл\n",
    "def save_audio(audio, filename, sample_rate=22050):\n",
    "    sf.write(filename, audio, sample_rate)\n",
    "\n",
    "# Пример использования\n",
    "text = \"Hello, this is a test.\"\n",
    "audio = text_to_audio(text, model)\n",
    "save_audio(audio, \"output_audio.wav\")\n"
   ],
   "id": "53b389baa76827e7",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " LSTM + Spectrogram-based",
   "id": "b47778e4139ce9d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-28T08:31:18.017493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "# Модель LSTM для преобразования текста в спектрограмму\n",
    "class TextToSpectrogramLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TextToSpectrogramLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        # Изменяем размерность на выходе с 128 на 256, чтобы соответствовать размеру мел-спектрограммы\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Функция для преобразования аудио в мел-спектрограмму\n",
    "def audio_to_mel(audio_path):\n",
    "    # Загрузка аудиофайла\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    # Вычисление мел-спектрограммы с 256 мел-частотными бинами\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=256)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Транспонируем, чтобы размерность была (время, признаки)\n",
    "    return mel_spectrogram.T\n",
    "\n",
    "# Пример тренировки модели\n",
    "def train_lstm(dataset_path, df):\n",
    "    model = TextToSpectrogramLSTM(input_size=256, hidden_size=512, output_size=256)  # Устанавливаем output_size=256\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Тренировка\n",
    "    for epoch in range(10):  # Количество эпох\n",
    "        for _, row in df.iterrows():\n",
    "            wav_file = f'data/test/normal_voices/{row[\"file\"]}.wav'  # Читаем путь к файлу\n",
    "            text = row['text']  # Текст (пока не используется, но можно добавить)\n",
    "\n",
    "            # Преобразование аудио в спектрограмму\n",
    "            mel = audio_to_mel(wav_file)\n",
    "            mel = torch.tensor(mel).unsqueeze(0).float()  # добавляем размерность для батча\n",
    "\n",
    "            # Прогон через модель\n",
    "            output = model(mel)\n",
    "            loss = criterion(output, mel)  # Сравниваем с мел-спектрограммой\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# Запуск тренировки\n",
    "train_lstm(dataset_path, df)\n"
   ],
   "id": "128854c8ace3af92",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/makarov/miniconda3/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
